{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39beea21",
   "metadata": {},
   "source": [
    "# 대규모 데이터 학습\n",
    "\n",
    "** 정의 **\n",
    "- 빅데이터는 메모리 등의 문제로 특정한 모형은 사용할 수 없는 경우가 있다.\n",
    "- 이러한 경우 다음의 모형을 사용하여 전체 데이터를 조각으로 나누어 학습하는 점진적 학습 방법을 사용할수 있다.  \n",
    "   - 사전확률분포를 설정할 수 있는 생성 모형\n",
    "   - 시작 가중치를 설정할 수 있는 모형\n",
    "- 모형의 특성에 따라서 partial_fit()의 기능이 조금씩 다르다.\n",
    "- SGDClassifier() : 확률적 경사하강법 모형\n",
    "   - model.partial_fit(X, y, classes=classes)\n",
    "   - partial_fit() 을 사용하여 경사하강법을 단계적으로 시행한 후 다음 단계에서 가중치를 사용할 수 있다. \n",
    "   - 1 epoch 당 10번의 split 데이터를 학습하여 누적된 모형의 점수를 반환할 수 있다.\n",
    "- NB\n",
    "   - clf.partial_fit(X, y, classes=classes) : 1 batch sample을 학습한 확률분포를 다음 훈련 모형의 사전확률분포로 사용할 수 있다. \n",
    "   - SGDC와 방식이 다르다.\n",
    "- 그래디언트 부스팅 : LightGBM\n",
    "   - lightgbm을 사용하면 초기 커미티 멤버로 일부 데이터를 학습한 모형을 사용할 수 있다.\n",
    "   - lightgbm의 train() 클래스를 사용한다. 특정 모형을 구현하지 않고, train() 클래스에서 params와 데이터 등을 정의 한다.\n",
    "   - 내장 된 모형이 있는 것 같다.\n",
    "- RF ensemble\n",
    "   - RF 등의 ensemble 모형에서는 일부 데이터를 사용한 모형을 개별 분류기로 사용할 수 있다. \n",
    "   - clf.fit(X, y)\n",
    "   - clf.n_estimators += n_tree_step\n",
    "- 빅데이터의 예측 모형은 메모리 등 컴퓨터 자원을 효율적으로 활용하기 위하여 학습 데이터를 조각하여 훈련한다.\n",
    "   - 모형의 매서드로 모형을 누적하여 fitting 해준다. 누적 fitting 방식은 모형마다 다르다.\n",
    "   - SGDClassifier\n",
    "   - NB\n",
    "   - Gradient Boosting\n",
    "   - RF\n",
    "- 데이터를 어떻게 분할하여 학습하고 지표를 반환할 것인지 알고리즘 적 판단을 해야할 것 같다.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2fe503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81db726a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((581012, 54), (581012,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covtype = fetch_covtype(shuffle=True, random_state=0)\n",
    "X_covtype = covtype.data\n",
    "y_covtype = covtype.target\n",
    "\n",
    "X_covtype.shape, y_covtype.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949a05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_covtype, y_covtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f97661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((435759, 54), (145253, 54), (435759,), (145253,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca044f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.unique(y_covtype)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "463ddcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "486ba55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12127056091780815"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f68896ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_Xy(start, end) : \n",
    "    \n",
    "    # strat ~ y_train 길이와 end 중 작은값 까지\n",
    "    idx = list(range(start, min(len(y_train) -1, end)))\n",
    "    X = X_train[idx, :]\n",
    "    y = y_train[idx]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550ddb8",
   "metadata": {},
   "source": [
    "## SGD 사용\n",
    "- 퍼셉트론 모형은 가중치를 계속 업데이트한다.\n",
    "- 일부 데이터를 사용하여 가중치를 구하고, 다음 단계에서 초기 가중치로 사용가능\n",
    "- 1 epoch 당 10번의 split 학습 데이터를 fitting 하고 score 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12e64a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 train acc=0.705 test acc=0.706\n",
      "epoch=1 train acc=0.705 test acc=0.705\n",
      "epoch=2 train acc=0.705 test acc=0.705\n",
      "epoch=3 train acc=0.705 test acc=0.705\n",
      "epoch=4 train acc=0.707 test acc=0.707\n",
      "epoch=5 train acc=0.707 test acc=0.708\n",
      "epoch=6 train acc=0.707 test acc=0.708\n",
      "epoch=7 train acc=0.708 test acc=0.708\n",
      "epoch=8 train acc=0.708 test acc=0.709\n",
      "epoch=9 train acc=0.709 test acc=0.709\n",
      "CPU times: total: 15.3 s\n",
      "Wall time: 8.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = SGDClassifier(random_state=0)\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "n_epoch = 10\n",
    "for epoch in range(n_epoch) : \n",
    "    for n in range(n_split) : \n",
    "        ## start : 0~9 순차적으로 n_X와 곱해진다.\n",
    "        start = n * n_X  \n",
    "        ## end : 1~10 순차적으로 n_X와 곱해진다.\n",
    "        end = (n+1) * n_X  \n",
    "        ## start, end로 만든 idx로 X, y 반환\n",
    "        X, y = read_Xy(start, end)\n",
    "        ## partial_fit : one epoch 경사하강법 수행 \n",
    "        model.partial_fit(X, y, classes=classes)\n",
    "    accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    accuracy_test = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"epoch={:d} train acc={:5.3f} test acc={:5.3f}\"\\\n",
    "          .format(epoch, accuracy_train, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2064948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 43575\n",
      "1 2 43575 87150\n",
      "2 3 87150 130725\n",
      "3 4 130725 174300\n",
      "4 5 174300 217875\n",
      "5 6 217875 261450\n",
      "6 7 261450 305025\n",
      "7 8 305025 348600\n",
      "8 9 348600 392175\n",
      "9 10 392175 435750\n"
     ]
    }
   ],
   "source": [
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "for n in range(n_split) : \n",
    "    start = n * n_X\n",
    "    end = (n+1) * n_X\n",
    "    print(n, n+1, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df5e95",
   "metadata": {},
   "source": [
    "## Naive Bayes \n",
    "- clf.partial_fit(X, y, classes=classes)\n",
    "   - batch 사이즈의 샘플에 대해 증분 fitting 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a4bd13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=0 train acc=0.635 test acc=0.635\n",
      "n=1 train acc=0.634 test acc=0.634\n",
      "n=2 train acc=0.632 test acc=0.632\n",
      "n=3 train acc=0.632 test acc=0.632\n",
      "n=4 train acc=0.633 test acc=0.633\n",
      "n=5 train acc=0.632 test acc=0.633\n",
      "n=6 train acc=0.632 test acc=0.632\n",
      "n=7 train acc=0.632 test acc=0.632\n",
      "n=8 train acc=0.631 test acc=0.632\n",
      "n=9 train acc=0.632 test acc=0.632\n",
      "CPU times: total: 15 s\n",
      "Wall time: 3.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "model = BernoulliNB()\n",
    "\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "for n in range(n_split) : \n",
    "    start = n * n_X\n",
    "    end = (n+1) * n_X\n",
    "    X, y = read_Xy(start, end)\n",
    "    model.partial_fit(X, y, classes=classes)\n",
    "    acc_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    acc_test = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"n={:d} train acc={:5.3f} test acc={:5.3f}\".format(n, acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "723ec3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 train acc=0.632 test acc=0.632\n",
      "epoch=1 train acc=0.632 test acc=0.632\n",
      "epoch=2 train acc=0.632 test acc=0.632\n",
      "epoch=3 train acc=0.632 test acc=0.632\n",
      "epoch=4 train acc=0.632 test acc=0.632\n",
      "epoch=5 train acc=0.632 test acc=0.632\n",
      "epoch=6 train acc=0.632 test acc=0.632\n",
      "epoch=7 train acc=0.632 test acc=0.632\n",
      "epoch=8 train acc=0.632 test acc=0.632\n",
      "epoch=9 train acc=0.632 test acc=0.632\n",
      "CPU times: total: 28.9 s\n",
      "Wall time: 7.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "model = BernoulliNB()\n",
    "\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "n_epoch = 10\n",
    "for epoch in range(n_epoch) : \n",
    "    for n in range(n_split) : \n",
    "        start = n * n_X\n",
    "        end = (n+1) * n_X\n",
    "        X, y = read_Xy(start, end)\n",
    "        model.partial_fit(X, y, classes=classes)\n",
    "    acc_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    acc_test = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"epoch={:d} train acc={:5.3f} test acc={:5.3f}\".format(epoch, acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6dfa3",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "- 초기 커미티 멤버로 일부 데이터를 학습한 모형을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b72c59cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at C:\\DS\\Anaconda3\\envs\\dev_env:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "lightgbm                  4.1.0           py310h5da7b33_0  \n"
     ]
    }
   ],
   "source": [
    "!conda list \"^light\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e65c5fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2202\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -1.009549\n",
      "[LightGBM] [Info] Start training from score -0.718597\n",
      "[LightGBM] [Info] Start training from score -2.783828\n",
      "[LightGBM] [Info] Start training from score -5.344701\n",
      "[LightGBM] [Info] Start training from score -4.135453\n",
      "[LightGBM] [Info] Start training from score -3.511350\n",
      "[LightGBM] [Info] Start training from score -3.331081\n",
      "n=0 train acc=0.780 test acc=0.779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2201\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "n=1 train acc=0.804 test acc=0.802\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2195\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 49\n",
      "n=2 train acc=0.806 test acc=0.802\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2207\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 51\n",
      "n=3 train acc=0.805 test acc=0.802\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2206\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "n=4 train acc=0.805 test acc=0.800\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2198\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 49\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=5 train acc=0.795 test acc=0.790\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2202\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 49\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=6 train acc=0.801 test acc=0.795\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2210\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 51\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=7 train acc=0.807 test acc=0.802\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2200\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=8 train acc=0.806 test acc=0.800\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2203\n",
      "[LightGBM] [Info] Number of data points in the train set: 43575, number of used features: 50\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "n=9 train acc=0.807 test acc=0.801\n",
      "CPU times: total: 4min 40s\n",
      "Wall time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from lightgbm import train, Dataset\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": len(classes)+1,\n",
    "    \"learning_rate\": 0.2,\n",
    "    \"seed\": 0\n",
    "}\n",
    "\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "num_tree = 10\n",
    "model = None\n",
    "for n in range(n_split) : \n",
    "    start = n * n_X\n",
    "    end = (n+1) * n_X\n",
    "    \n",
    "    X, y = read_Xy(start, end)\n",
    "    model = train(params, \n",
    "                  init_model=model, \n",
    "                  train_set=Dataset(X, y),\n",
    "                  keep_training_booster=False, \n",
    "                  num_boost_round=num_tree)\n",
    "    \n",
    "    acc_train = accuracy_score(y_train, np.argmax(model.predict(X_train), axis=1))\n",
    "    acc_test = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1))\n",
    "    \n",
    "    print(\"n={:d} train acc={:5.3f} test acc={:5.3f}\"\\\n",
    "         .format(n, acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbfe8ed",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20950e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 train acc=0.868 test acc=0.854\n",
      "epoch=1 train acc=0.891 test acc=0.872\n",
      "epoch=2 train acc=0.899 test acc=0.879\n",
      "epoch=3 train acc=0.902 test acc=0.883\n",
      "epoch=4 train acc=0.904 test acc=0.885\n",
      "epoch=5 train acc=0.905 test acc=0.886\n",
      "epoch=6 train acc=0.906 test acc=0.887\n",
      "epoch=7 train acc=0.906 test acc=0.887\n",
      "epoch=8 train acc=0.907 test acc=0.888\n",
      "epoch=9 train acc=0.907 test acc=0.889\n",
      "CPU times: total: 1min 22s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_split = 10\n",
    "n_X = len(y_train) // n_split\n",
    "num_tree_ini = 10\n",
    "num_tree_step = 10\n",
    "model = RandomForestClassifier(n_estimators=num_tree_ini, warm_start=True)\n",
    "for n in range(n_split) : \n",
    "    start = n * n_X\n",
    "    end = (n+1) * n_X\n",
    "    \n",
    "    X, y = read_Xy(start, end)\n",
    "    model.fit(X, y)\n",
    "    acc_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    acc_test = accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    print(\"epoch={:d} train acc={:5.3f} test acc={:5.3f}\"\n",
    "          .format(n, acc_train, acc_test))\n",
    "    \n",
    "    model.n_estimators += num_tree_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
